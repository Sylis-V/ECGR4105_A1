# -*- coding: utf-8 -*-
"""ECGR 4105.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18ONd_VNJ3D1gK4Rkfa93H-29N0LMvn3C
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Load the data
data = pd.read_csv('D3.csv')
X = data[['X1', 'X2', 'X3']].values
y = data['Y'].values

# Set the learning rate and number of iterations
alpha = 0.05
iterations = 1000

# Define the linear regression function
def linear_regression(X, y, alpha, iterations):
    m = len(y)
    X = np.concatenate((np.ones((m, 1)), X), axis=1)  # Add bias term
    theta = np.zeros(X.shape[1])  # Initialize theta to zero
    cost_history = np.zeros(iterations)

    for i in range(iterations):
        h = np.dot(X, theta)  # Hypothesis
        loss = h - y  # Calculate the error
        gradient = np.dot(X.T, loss) / m  # Compute the gradient
        theta -= alpha * gradient  # Update theta
        cost = np.sum(loss ** 2) / (2 * m)  # Calculate the cost
        cost_history[i] = cost

    return theta, cost_history

# Train the model for each explanatory variable in isolation
theta_history = []
cost_history = []

for i in range(X.shape[1]):
    X_train = X[:, i].reshape(-1, 1)  # Select one explanatory variable
    theta, cost = linear_regression(X_train, y, alpha, iterations)
    theta_history.append(theta)
    cost_history.append(cost)

# Print the results
for i in range(X.shape[1]):
    print(f'Theta for X{i+1}: {theta_history[i]}')

# Plot the cost history
plt.figure(figsize=(10, 5))
for i in range(X.shape[1]):
    plt.plot(theta_history[i], label=f'X{i+1}')

plt.xlabel('Iterations')
plt.ylabel('Cost')
plt.title('Cost History')
plt.legend()
plt.show()

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Load the data
data = pd.read_csv('D3.csv')
X = data[['X1', 'X2', 'X3']].values
y = data['Y'].values

# Set the learning rate and number of iterations
alpha = 0.01  # You can experiment with different values like 0.1
iterations = 1000

# Define the linear regression function
def linear_regression(X, y, alpha, iterations):
    m = len(y)
    X = np.concatenate((np.ones((m, 1)), X), axis=1)  # Add bias term
    theta = np.zeros(X.shape[1])  # Initialize theta to zero
    cost_history = np.zeros(iterations)

    for i in range(iterations):
        h = np.dot(X, theta)  # Hypothesis
        loss = h - y  # Calculate the error
        gradient = np.dot(X.T, loss) / m  # Compute the gradient
        theta -= alpha * gradient  # Update theta
        cost = np.sum(loss ** 2) / (2 * m)  # Calculate the cost
        cost_history[i] = cost

    return theta, cost_history

# Train the model for each explanatory variable in isolation
theta_history = []
cost_history = []
X_train_list = []  # Store the training data for plotting

for i in range(X.shape[1]):
    X_train = X[:, i].reshape(-1, 1)  # Select one explanatory variable
    X_train_list.append(X_train) # Store the training data
    theta, cost = linear_regression(X_train, y, alpha, iterations)
    theta_history.append(theta)
    cost_history.append(cost)

# Report the linear models
print("Linear Models:")
for i in range(X.shape[1]):
    print(f"Y = {theta_history[i][0]:.4f} + {theta_history[i][1]:.4f} * X{i+1}")

# Plot the final regression model and loss over iteration
plt.figure(figsize=(15, 5))

for i in range(X.shape[1]):
    plt.subplot(1, 3, i + 1)  # Create subplots for each variable
    plt.scatter(X_train_list[i], y, label=f'Data X{i+1}') # Scatter plot of the data

    # Plot the regression line
    x_vals = np.linspace(X_train_list[i].min(), X_train_list[i].max(), 100)
    X_vals_with_bias = np.concatenate((np.ones((100, 1)), x_vals.reshape(-1,1)), axis = 1)
    y_vals = np.dot(X_vals_with_bias, theta_history[i])
    plt.plot(x_vals, y_vals, color='red', label=f'Regression Line X{i+1}')

    plt.xlabel(f'X{i+1}')
    plt.ylabel('Y')
    plt.title(f'Regression Model for X{i+1}')
    plt.legend()

    # Plot the cost history
    plt.figure(figsize=(15,5))
    plt.plot(cost_history[i])
    plt.xlabel('Iterations')
    plt.ylabel('Cost')
    plt.title(f'Cost History for X{i+1}')
    plt.show() # Show the cost history plot immediately

plt.tight_layout()
plt.show() # Show the regression model plots

# Find the explanatory variable with the lowest loss
final_costs = [cost[-1] for cost in cost_history]
best_variable_index = np.argmin(final_costs)
print(f"\nExplanatory variable X{best_variable_index + 1} has the lowest final loss: {final_costs[best_variable_index]:.4f}")

# Impact of learning rate
print("\nImpact of Learning Rate (Observations):")
print("A higher learning rate (e.g., 0.1) can lead to faster convergence but may also overshoot the minimum and oscillate or diverge. A lower learning rate (e.g., 0.01) is more stable but may converge slower, requiring more iterations. The optimal learning rate depends on the specific data and problem.")

import pandas as pd
import matplotlib.pyplot as plt

# Load the data
data = pd.read_csv('D3.csv')

# Extract the first and last columns
X_1 = data.iloc[:, 0].values  # First column
X_2 = data.iloc[:, 1].values  # Second column
X_3 = data.iloc[:, 2].values  # Third column
Y = data.iloc[:, 3].values # Fourth column

theta_1 = 0.5
theta_2 = 0.5
theta_3 = 0.5
alpha = 0.1
iterations = 1000

m = len(Y)
h = np.zeros(len(Y))

for i in range(iterations):
    h1 = theta_1 * X_1  # Vectorized calculation of h
    loss1 = h1 - Y
    h2 = theta_2 * X_2
    loss2 = h2 - Y
    h3 = theta_3 * X_3
    loss3 = h3 - Y

    # Calculate the gradients (vectorized)
    d_theta_1 = (1/m) * np.sum(loss1 * X_1)
    d_theta_2 = (1/m) * np.sum(loss2 * X_2)
    d_theta_3 = (1/m) * np.sum(loss3 * X_3)

    # Update theta (simultaneously)
    theta_1 = theta_1 - alpha * d_theta_1
    theta_2 = theta_2 - alpha * d_theta_2
    theta_3 = theta_3 - alpha * d_theta_3

    # (Optional) Print the loss (or cost) to monitor convergence
    cost1 = (1/(2*m)) * np.sum(loss1**2) # mean squared error
    cost2 = (1/(2*m)) * np.sum(loss2**2)
    cost3 = (1/(2*m)) * np.sum(loss3**2)
    if i % 100 == 0:  # Print every 100 iterations
        print(f"Iteration {i}: Cost = {cost1}")
        print(f"Iteration {i}: Cost = {cost2}")
        print(f"Iteration {i}: Cost = {cost3}")


print(f"Final theta_1: {theta_1}")
print(f"Final theta_2: {theta_2}")
print(f"Final theta_3: {theta_3}")

# Example prediction:
x_new_1, x_new_2, x_new_3 = 1.1, 2.2, 0.5
prediction = theta_1 * x_new_1 + theta_2 * x_new_2 + theta_3 * x_new_3
print(f"Prediction for {x_new_1}, {x_new_2}, {x_new_3}: {prediction}")

plt.figure(figsize=(15, 5))

plt.subplot(1, 3, 1)
plt.scatter(X_1, Y)
plt.xlabel("X_1")
plt.ylabel("Y")
plt.title("X_1 vs Y")

plt.subplot(1, 3, 2)
plt.scatter(X_2, Y)
plt.xlabel("X_2")
plt.ylabel("Y")
plt.title("X_2 vs Y")

plt.subplot(1, 3, 3)
plt.scatter(X_3, Y)
plt.xlabel("X_3")
plt.ylabel("Y")
plt.title("X_3 vs Y")

plt.tight_layout()
plt.show()

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Load the data
data = pd.read_csv('D3.csv')
X = data[['X1', 'X2', 'X3']].values
y = data['Y'].values

# Set the learning rate and number of iterations
alpha = 0.01  # You can experiment with different values like 0.1
iterations = 1000

# Define the linear regression function (modified to return theta history)
def linear_regression(X, y, alpha, iterations):
    m = len(y)
    X = np.concatenate((np.ones((m, 1)), X), axis=1)  # Add bias term
    theta = np.zeros(X.shape[1])  # Initialize theta to zero
    cost_history = np.zeros(iterations)
    theta_history = np.zeros((iterations, X.shape[1]))  # Store theta history

    for i in range(iterations):
        h = np.dot(X, theta)  # Hypothesis
        loss = h - y  # Calculate the error
        gradient = np.dot(X.T, loss) / m  # Compute the gradient
        theta -= alpha * gradient  # Update theta
        cost = np.sum(loss**2) / (2 * m)  # Calculate the cost (for monitoring)
        cost_history[i] = cost
        theta_history[i] = theta.copy()  # Store theta at each iteration

    return theta, cost_history, theta_history

# Train the model for each explanatory variable in isolation and plot
theta_history_all = []
loss_history_all = []  # Store loss history
X_train_list = []  # Store the training data for plotting

for i in range(X.shape[1]):
    X_train = X[:, i].reshape(-1, 1)  # Select one explanatory variable
    X_train_list.append(X_train)  # Store the training data
    theta, cost, theta_history = linear_regression(X_train, y, alpha, iterations)
    theta_history_all.append(theta_history)

    # Calculate loss for the last iteration
    X_train_with_bias = np.concatenate((np.ones((len(X_train), 1)), X_train), axis=1)
    h = np.dot(X_train_with_bias, theta_history[-1]) # Use the final theta values
    loss = h - y
    loss_history_all.append(loss)

    # Plot the loss (separate plot)
    plt.figure()  # Create a new figure for each plot
    plt.scatter(X_train, loss)  # Scatter plot of X vs. loss
    plt.xlabel(f"X{i+1}")
    plt.ylabel("Loss")
    plt.title(f"Loss vs. X{i+1} (Final Iteration)")
    plt.show()

    # Plot the final regression model (separate plot)
    y_pred = np.dot(X_train_with_bias, theta)  # Use the final theta
    plt.figure()  # Create a new figure for each plot
    plt.scatter(X_train, y, label="Data")
    plt.plot(X_train, y_pred, color='red', label="Regression Line")
    plt.xlabel(f"X{i+1}")
    plt.ylabel("Y")
    plt.title(f"Regression Line (X{i+1})")
    plt.legend()
    plt.show()

# Report the linear models
print("Linear Models:")
for i in range(X.shape[1]):
    print(f"Y = {theta_history_all[i][-1][0]:.4f} + {theta_history_all[i][-1][1]:.4f} * X{i+1}")  # Access last theta values

